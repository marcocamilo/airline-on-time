---
title: Data Collection
format: 
  html:
    code-fold: true
jupyter: python3
execute: 
  cache: true
---

## Introduction

Welcome to the first notebook in the series "EDA of Airline On-Time Performance". In this project, I will explore the data from the US Bureau of Transportation to gain insights into the performance of airlines.

The goal of this notebook is to collect and format the data for use in future notebooks. First, I will import the necessary packages and then download the data from the US Bureau of Transportation. I will then format the data to make it easier to work with in future notebooks. The Parket file format will be used to store the data, as it is a columnar storage format that is highly efficient for storing and querying large datasets.

Let's dive into each step in detail and get started with our analysis!

## Package Import

In this cell, the necessary Python libraries are imported for the data collection process. The libraries being imported are:

This logging configuration will help track events during the data collection process.

- `logging`: used to record events that occur while the software is running.

- `os`: provides a way for interacting with the file system and manipulating files or directory.

- `requests`: used for making HTTP requests in Python.

- `tarfile`: used to read and write tar archives, which combine multiple files into one.

- `pandas`: an open-source data analysis and manipulation library for Python.

- `tqdm`: a library that provides a fast, extensible progress bar for loops and other iterable objects in Python.

After importing the libraries, the logging configuration is set up using the basicConfig() function from the logging module. The root logger is configured with a level of INFO, meaning it will handle messages with a severity level of INFO and above. The format of the log messages is also specified. This will help keep track of the events when running the data collection process.

```{python}
import logging
import os
import requests
import tarfile
import pandas as pd
from tqdm import tqdm

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
```

## Data Collection

In this cell, the data is downloaded and saved from the US Bureau of Transportation. The code performs the following steps:

1. Defines the URL and dataset name to be downloaded, and the path where the data will be saved.
2. Downloads the dataset using the `requests` library.
3. Checks if the request is successful, before proceeding with the extraction. If not, it logs an error message.
4. Extracts the tar file with `tarfile` to the specified path using `os`.
5. Reads the extracted CSV file into a pandas DataFrame, specifying the encoding and data types for certain column.
6. Saves the DataFrame as a Parquet file in the specified path.
7. Removes the original CSV file to save storage space. Logs a message indicating that the data has been successfully saved.

```{python}
file = 'airline_2m.tar.gz'
url = 'https://dax-cdn.cdn.appdomain.cloud/dax-airline/1.0.1/' + file
path = "./data/raw/"
f_name = 'airline_2m'

logging.info("Starting request...")

response = requests.get(url, stream=True)
if response.status_code == 200:
    total_size = int(response.headers.get('content-length', 0))
    block_size = 8192  # 8 Kibibytes

    tar_path = os.path.join(path, file)
    with open(tar_path, "wb") as f:
        for data in tqdm(response.iter_content(block_size), total=total_size // block_size, unit='KiB', unit_scale=True):
            f.write(data)

    logging.info("Opening tar file...")
    with tarfile.open(tar_path, "r") as tf:
        tf.extractall(path=path)

    logging.info("Reading and formatting data...")
    csv_path = os.path.join(path, f_name + '.csv')

    # Reading the CSV with a progress bar
    csv_reader = pd.read_csv(csv_path, encoding="ISO-8859-1",
                             dtype={'Div1Airport': 'category', 'Div1TailNum': 'category', 
                                    'Div2Airport': 'category', 'Div2TailNum': 'category'},
                             chunksize=10000)  # Reading in chunks

    df_list = []
    for chunk in tqdm(csv_reader, unit='chunk'):
        df_list.append(chunk)
    
    df = pd.concat(df_list, ignore_index=True)

    logging.info("Saving data as parquet file...")
    parquet_path = os.path.join(path, f_name + '.parquet')
    
    # Writing the parquet file with progress bar
    df.to_parquet(parquet_path, engine='pyarrow')

    logging.info("Removing temporary files...")
    os.remove(csv_path)
    os.remove(tar_path)

    logging.info("Data successfully saved!")
else:
    logging.info("Failed to download the dataset.")
```
