---
title: Univariate Analysis
format: 
  html:
    code-fold: true
jupyter: python3
execute: 
  cache: true
---

## Introduction

## Importing Packages and Data

```{python}
#| code-fold: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from src.modules.analysis import univariate_preview
from src.modules.variables_03 import *

figsize = (14,8)

plt.rcParams['figure.figsize'] = figsize
sns.set(rc={'figure.figsize':figsize})
pd.set_option('display.max_columns', 200)
pd.options.display.float_format = '{:,.3f}'.format

df = pd.read_parquet('./data/interim/03-airline_2m.parquet')
```


## Time Variables

We decide to analyze the time variables (`CRSDepTime`, `DepTime`, `CRSArrTime`, `ArrTime`) separate from the delay variables, as they require different data cleaning tasks and provide deeper insights into the distribution of the data. We will start by analyzing the distribution of these variables and determine any potential issues that may require further processing.

### Value Counts

```{python}
time_cols = ['CRSDepTime', 'DepTime', 'CRSArrTime', 'ArrTime']
colors = ['red', 'green', 'yellow', 'blue']

time_hist(df, time_cols, colors, 75, 'Distribution of CRS and True Depature and Arrivals')
time_hist(df, time_cols, colors, 100, 'Distribution of CRS and True Depature and Arrivals: Zoomed into Values 0-100', limit=500)
```

The histograms above reveal several insights about the nature of time variables in the dataset. Firstly, we see the distribution is not continuous, but discrete with regular gaps throughout the range of values. This is probably due to the HHMM format, which, representing time values, excludes all values between 60 and 99. This results in a discontinous range of values, which is not ideal for out analysis.

Moreover, there is a markedly lower frequency of values below 500, showing that flights before 500 are less frequent. The second histogram zooms into this range. The plot not only demosntrates the progression in the number of *red eye* flights between midnight and 5am, but also shows more clearly that the distribution of values is regularly discontinuous at values between 60 and 99. This confirms that HHMM is not a suitable format for time operations, and is instead must be replaced by a `datetime` object. Moreover, late-night `ArrTime` values in particular seem to be the most frequent among the time variables.

Finally, there is a markedly large number of 0s among time variables. This could suggest that missing values are expressed by means of a 0 value. This might represent a two-fold issue: 0 is representing missing values, while 2400 represents midnight in an invalid time format. This results in midnight values and missing values are undistinguishable in the dataset. This will require some processing before converting values to a `datetime` object.

We will now address these issues in the following sections.

### Understanding 0s, 2400s and NaNs

We will start by exploring the value counts of 0s, 2400s and missing values in the time variables. We will then decide how to handle these values.

```{python}
pd.DataFrame({
    '0': df[time_cols].eq(0).sum(),
    'na': df[time_cols].isna().sum(),
    '2400': df[time_cols].eq(2400).sum()
})
```

The value counts show some interesting patterns. Both CRS variables `CRSDepTime` and `CRSArrTime` have the *exact* same number of 0s and do not contain missing values. This makes it highly likely that these values represent missing values. Additionally, the 2400 values occur mostly in true time variables and tend more to be arrival times. They are likely to represent midnight, but are not supported by the `datetime` object.

Given the spike of 0 values shown in the histogram above, along with the complete absence of NaN values in the CRS variables, we can conclude that the 0s in the CRS variables represent missing values. We will deal with how to fill these values in a later section.

### Feature Engineering: Datetime Conversion

As we stated above, the HHMM format is not a suitable format for time operations, and is instead must be replaced by a `datetime` object. We will start by converting the time variables to a `datetime` object. This will allow us to perform date operations and visualizations.

However, in order to do this, we must also account for the 0s and 2400s in the CRS variables, as well as the NaN values in the true time variables. We will create a function that will convert the time variables to a `datetime` object, while also accounting for these values.

```{python}
def convert_to_datetime(df, time_cols, date):
    matrix = df[time_cols].values
    matrix = matrix.astype('int').astype('str')
    matrix = np.char.zfill(matrix, 4)
    matrix[matrix == '0000'] = np.nan
    matrix[matrix == '2400'] = '0000'
    df[time_cols] = matrix
    for col in time_cols:
        df[col] = pd.to_datetime(date + ' ' + df[col], format="%Y-%m-%d %H%M", errors='coerce')
    return df

df = convert_to_datetime(df, time_cols, df['FlightDate'].astype(str))

df[time_cols]
```

Now, all values are in a valid `datetime` format, and in a continuous range of values. Moreover, all 0 values are now uniformely represented by NaN values. Below we print the missing value counts for the CRS variables, to confirm they have the same value count as 0s before this transformation.

```{python}
df[time_cols].isna().sum()
```

### Data Cleaning: Filling Missing CRS Values

We will now consider how to best fill in the missing CRS values. Our first option is to attempt to fill them using information from other variables. We will start by exploring the rows with missing time values to see if there is any pattern that could help us fill them.

```{python}
df[df[time_cols].isna().any(axis=1)][dep_cols + arr_cols].sample(10)
```

From the rows sample we can see that flights with CRS missing time values all have the rest of their departure and arrival information. Although we can see from the above value counts that delay variables still have a small amount of missing values, we can consider calculating the missing values from the true departure and arrival times, and the delay variables. This will allow us to fill the missing values with the sum of the true time and the delay.

Before we proceed, lets drop the rows with missing delay time values, since their miniscule amount will not affect our analysis.

```{python}
print(f"Before delay drop: {df.shape}")
df = df.dropna(subset=dep_cols[1:] + arr_cols[1:])
print(f"After delay drop: {df.shape}")
```

Now, we will fill the missing CRS values by using the `.fillna()` function and use the sum of the true time and the delay as the fill-in value. We will do this for both the `DepTime` and `ArrTime` variables, and print the missing value counts for the time variables to confirm the changes.

```{python}
df['CRSDepTime'] = df['CRSDepTime'].fillna(df['DepTime'] - pd.to_timedelta(df['DepDelay'], unit='m'))
df['CRSArrTime'] = df['CRSArrTime'].fillna(df['ArrTime'] - pd.to_timedelta(df['ArrDelay'], unit='m'))

df[time_cols].isna().sum()
```

### Data Cleaning: Imputing Date and Time Zones for Datetime Objects

Before moving on, we must address an unfinished issue. Despite the datatype conversion of time variables to `datetime`, all time variables contain the same date, namely `FlightDate`. For calculating time differences, this is not an issue in same-day flights. However, for cases where the flight spans midnight or across time zones, this could be a problem. Moreover, the times are in local time and do not contain time zone information. To solve this problem, we will need to add the timezone imformation to then calculate accurate dates for the rest of the time columns.

In order to accurately add the flight dates, we will need to add the time zone for all times in the dataset. For this we will use two datasets, one using the [`airportsdata`](https://github.com/mborsetti/airportsdata) package and another using a dataset from the [Airports API](https://github.com/ryanburnette/airports-api). We will merge these datasets to create a dictionary that maps each airport to its corresponding time zone, which we will then use to assign time zones to the time variables.

```{python}
import airportsdata
airports1 = {airport: info['tz'] for airport, info in airportsdata.load('IATA').items()}
airports1 = pd.DataFrame({
    'iata': airports1.keys(),
    'timezone': airports1.values()
})

airports2 = pd.read_csv('./data/external/airports.csv', usecols=['iata', 'timezone']).dropna()

airports_data = pd.concat([airports1, airports2])
airports_data = airports_data.drop_duplicates(subset='iata').set_index('iata')

airports_data = airports_data.loc[pd.concat([df['Origin'], df['Dest']]).unique()]
tz_dict = airports_data['timezone'].to_dict() 
```

With the time zone dictionary created, we will write a function that will add the time zones to the time variables. The function takes a time variable and its corresponding departure or arrival location variable as arguments. It first creates a series of time zones for each location. Then, it loops over each unique time zone and creates a mask that extracts the indices corresponding to that time zone. It then localizes all times found in those indices to the corresponding time zone. In case of any ambiguous or non-existent times, it sets them to `NaT` and drops any rows with missing values. We apply this function over all departure and arrival time variables.
```{python}
def time_zone(df, time_col, location_col):
    timezones = df[location_col].map(tz_dict)
    times = pd.to_datetime(df[time_col])
    
    unique_timezones = timezones.unique()
    for tz in unique_timezones:
        mask = timezones == tz
        df.loc[mask, time_col] = times[mask].dt.tz_localize(tz, ambiguous='NaT', nonexistent='NaT')
        df = df.dropna(subset=[time_col])
    
    return df

shape_before = df.shape[0]

for time_col in time_cols[:2]:
    df = time_zone(df, time_col, 'Origin')
for time_col in time_cols[2:]:
    df = time_zone(df, time_col, 'Dest')

time_loc_cols = ['Origin'] + time_cols[:2] + ['Dest'] + time_cols[2:]
display(df[time_loc_cols].head())

shape_after = df.shape[0]

print(f"Dataframe reduction: {(shape_before - shape_after)/shape_before:.2%}")
```

Now that all times have their corresponding time zones, we can accurately calculate the dates for all time variables. We assume that the `FlightDate` only applies to the scheduled flight departure. Consequently, all other time variables will have to be calculated based on the `FlightDate` and the time difference between the `CRSDepTime` and the rest of the time variables. To achieve this, we will write a function that takes two columns: the time variable and its corresponding state variable (whether departure or arrival state). The function will convert the time variable to UTC and calculate the time difference between the variable and the `CRSDepTime`. Then, it will add this difference to the original time variable, which will result in an accurate date in UTC. Finally, the function will search the time zone of the corresponding state and convert the datetime object back to its original time zone.

```{python}
def date_calculation(df, time_col, location_col, deptime=False):
    timezones = df[location_col].map(tz_dict)
    times = df[time_col]
    
    utc_crs_dep = pd.to_datetime(df['CRSDepTime'], utc=True)
    utc_times = pd.to_datetime(times, utc=True)

    diff = (utc_times - utc_crs_dep).dt.seconds
    if deptime:
        diff -= (utc_times.dt.time < utc_crs_dep.dt.time) * 86400
    new_times = utc_crs_dep + pd.to_timedelta(diff, 's')
    
    unique_timezones = timezones.unique()
    for tz in unique_timezones:
        mask = timezones == tz
        df.loc[mask, time_col] = new_times[mask].dt.tz_convert(tz)
    
    return df

df = date_calculation(df, 'CRSArrTime', 'Dest')
df = date_calculation(df, 'ArrTime', 'Dest')
df = date_calculation(df, 'DepTime', 'Origin', True)
```

To make sure that the date calculations are accurate, we will check the dataset for arrival variables that are earlier than their corresponding departure variable.

```{python}
negative_crsarr = df.query("CRSArrTime < CRSDepTime")
negative_arr = df.query("ArrTime < DepTime")

print(f"Negative CRSArrTime values: {negative_crsarr.shape[0]/df.shape[0]:.2%} ({negative_crsarr.shape[0]})")
display(negative_crsarr[time_loc_cols])

print(f"Negative ArrTime values: {negative_arr.shape[0]/df.shape[0]:.2%} ({negative_arr.shape[0]})")
display(negative_arr[time_loc_cols])
```

There seem to be a few flights with arrival times earlier than their departure times, even in UTC time. For some cases, it is likely due to the fact that some US states have more than one time zone (as shown in the image below), while our timezone dictionary only accounts for one timezone per state. These are the cases for states such as Michigan, Winsconsin, North and South Dakota, Indiana, Texas, Teneessee, Kentucky, among others, which can be seen in the sample data above. For other cases, it is likely due to the fact that some flights span across midnight in UTC, and our function does not account for this. This can be seen in the sample data above, where `DepTime` has an additional day compared to `CRSDepTime`.

![](https://www.nationsonline.org/maps/US-timezones-map.jpg)

Since the number of such values is negligible, we will drop these rows from the dataset.

```{python}
print(f"Before drop of negative arrivals: {df.shape}")
df = df.drop(negative_arr.index)
print(f"After drop of negative arrivals: {df.shape}")
```

### Data Cleaning: Changing Data Types

A minor detail we mentioned above is that the numerical variables measuring duration and boolean variables would benefit from being saved as integers. We will thus convert the delay variables to integers and boolean variables.

```{python}
df[dep_cols[2:] + arr_cols[2:]] = df[dep_cols[2:] + arr_cols[2:]].astype('int')
df[dep_cols[2:] + arr_cols[2:]].sample(5)
```

## Departure and Arrival Variables (updated)

So far, we have explore missing value correlations among departure and arrival variables, dropped unnecessary variables, converted the time variables to `datetime` format, filled missing CRS values, and created binary delay variables. As mentioned at the start of this section, considering the need for data cleaning and the varying nature of missing values, we can now analyze the distribution of the departure and arrival variables in the dataset.

We will divide our analysis into two parts: the time variables and the delay variables. Given our data has been cleaned, we can finally focus on the statistical analysis of these variables.

### Time Variables

Since our time data is now clean and in a valid format, we will start by analyzing the distribution of the time variables in the dataset. We will use the `time_hist` function to create a histogram for the time variables.
```{python}
for col, color in zip(time_cols, colors):
    times = df[col].apply(lambda x : x.time())
    times = [t.hour * 60 + t.minute for t in times]
    plt.hist(times, alpha=0.5, bins=75, color=color)
    plt.axvline(np.mean(times), color=color, linestyle='dashed', linewidth=2)
plt.legend(time_cols)
plt.title('Distribution of CRS and True Depature and Arrivals')
plt.xticks(np.arange(0, 1441, 60), [f'{h}:00' for h in range(25)])
plt.show()
```

The histogram now shows a clear picture on the distribution of time variables. At first glance, the plot shows departures are more prevalent during morning hours, while arrivals are more predominant during the evening, which is expected. This is further demonstrated by the 2-hour difference in the depature and arrival means, which shows that departures tend to occur earlier on average than arrivals.

Particularly noticeable is the progression of flight frequencies across the day. The data shows that early morning flights, particularly at 3am, are the least frequent for both depatures and arrivals. However, there is a significant increase in the number of flights at 5am for departures and 7am for arrivals. There is a noticeable of departure flights at 7am, with over 55000 flights registered across the dataset. Afterwards, overall peak hours, where *both* arrivals and departures are at their highest, remain relatively consistent from 9pm. The number of departures decreases first at 6pm, whereas the number of arrivals starts decreasing from 9pm. This decrease continues past midnight, reaching its lowest point at around 3am.

Moreover, the distribution of CRS and true time variable pairs for departures and arrivals, respectively, are almost identical, as shown by the close pairs of means. This already reveals that the size of flight delays is small on average across the dataset. 

Given this insight, let us now focus on the distribution of delay variables in particular.

### Delay Variables

```{python}
univariate_preview(df, dep_cols[2:] + arr_cols[2:])
```

The value counts above show several insights about the delay variables. `DepDelay` and `ArrDelay` have the highest number of unique values, since they are continuous variables that include negative values (representing flights ahead of schedule). Interestingly, their top 5 values counts show that on-time and ahead of scheduled flights (by less than 5 minutes) are the most frequent. The delay minutes variables have a lower value count, since they only include positive values. According to the value counts, delays of less than 5 minutes are the most common for both departures and arrivals. The delay groups variables give a deeper glimpse into the amount of delays, as they count the number of 15-minute intervals for flight delays. The most frequent delay groups range from -15 to 30 minutes. The binary delay variables further confirm that the majority of flights are on time.

The summary statistics on the other hand show some concerning facts about the `DepDelay` and `ArrDelay` variables in particular. According to the summary statistics, the minimum delay is -990 minutes (or -16.5 hours) for departures and -706 minutes (or -11.8 hours) for arrivals. Conversely, the maximum delay for these, as well as for the delay minutes variables, is 1878 minutes or (31.3 hours) for departures and 1898 minutes (or 31.64 hours) for arrivals. This is highly concerning, as it is unlikely for flights to be delayed by such a large amount of time. This clearly indicates the presence of outliers in the dataset.

Before exploring this issue, let us first visualize the distribution of the categorical variables in this group to better understand what is really happening with flight delays in the dataset. We will start by creating histograms for the delay variables.

```{python}
variables = dep_cols[4:] + arr_cols[4:]
ncols = 2
nvars = len(variables)
fig, axes = plt.subplots(nrows=int(nvars/ncols), ncols=ncols, figsize=(20, 10), gridspec_kw={"width_ratios": (.4, .8)})

for col, ax in zip(dep_cols[4:] + arr_cols[4:], axes.flatten()):
    if df[col].nunique() > 2:
        ax.hist(df[col], bins=30, edgecolor='black')
        ax.set_title(col)
    else:
        sns.countplot(x=col, data=df, ax=ax, edgecolor='black', stat='percent')
        ax.bar_label(ax.containers[0], fmt="%.2f")
        ax.set_xlabel(None)
        ax.set_title(col)
    plt.tight_layout()
plt.savefig('./output/img/delay-cols.png')
plt.show()
```

The count plots for categorical delay variables give a better understanding as to why the delay variables have such large outliers. The delay binary variables show that around 80% of flights are on time, with only around 20% exhibiting some kind of delay. Morevoer, the 15-minute delay variables have a similar distribution, indicating that the majority of delayed flights are delayed by 15 minutes or more. This amount is considered as the threshold for a delayed flight by the airline industry.

The histograms for 15-minute delay groups offer a more detailed view of the distribution of delays, as shown with normalized counts below. Firstly, the distribution is right skewed, with more than 40% of flights ahead of schedule by less than 15 minutes and around 30% of flights on time. These two groups together represent around 70% of flights in the dataset. This is consistent with the value counts above, which show that the majority of flights exhibit no delays greater than 15 minutes (represented in these histograms by values less than or equal to 0). 

Beyond these values, we can better observe the outliers in the dataset. The distribution has a long, right tail, with an increasingly smaller number of flights experiencing delays greater than 15 minutes. A uniformly low count of flights can be seen for delays greater than three 15-minute intervals (or 45 minutes). These are the flights that are causing the large outliers in the delay variables. Finally, it is worth noting that the `ArrivalDelayGroups` variable even shows a significant amount of flights (around 13%) arriving ahead of schedule by more than 15 minutes. 

```{python}
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))

sns.histplot(df['DepartureDelayGroups'], bins=30, ax=axes[0], stat="percent", edgecolor='black')
sns.histplot(df['ArrivalDelayGroups'], bins=30, ax=axes[1], stat="percent", edgecolor='black')

plt.tight_layout()
plt.show()
```

We will leave the outlier exploration for these variables, as well as any other, for our next notebook covering the bivariate analysis of this dataset. For now, we will move on to the last sections, to analyze the distribution of the remaining variables in the dataset.

## Flight Summary Variables

We will now analyze the distribution of the flight summary variables, which include `FlightDate`, `UniqueCarrier`, `FlightNum`, `TailNum`, `Distance`, `DistanceGroup`, and `DivAirportLandings`. We will start by analyzing the distribution of these variables and determine any potential issues that may require further processing.

### Value Counts

```{python}
univariate_preview(df, sum_cols)
```

The value counts shows us that most variables except for `DistanceGroup` are numerical variables, thus exhibiting a high number of unique values. The values are mostly saved as floats, which, as with the departure and arrival variables in previous sections, might be best to save as integers. Regarding top values, we see that the most frequent values for the `Distance` column range from the low 200s to the high 300s, `AirTime` values cluster around 50 minutes, and `CRSElapsedTime` and `ActualElapsedTime` at 60 minutes. The `DistanceGroup` variable is a categorical variable that counts the flight distance in 250-mile intervals. All top values are less than 5, indicating that top flights are less than 1250 miles long. 

The summary statistics show a more detailed understanding of the numerical distribution of the variables, which we visualize below.
```{python}
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 10))
(ax1, ax2, ax3, ax4, ax5, ax6) = axes.flatten()

sns.histplot(df['Distance'], bins=30, ax=ax1, kde=True, edgecolor='black')
sns.countplot(x='DistanceGroup', data=df, ax=ax2, edgecolor='black', stat='percent')
sns.histplot(df['CRSElapsedTime'], bins=30, ax=ax3, kde=True, edgecolor='black')
sns.histplot(df['ActualElapsedTime'], bins=30, ax=ax4, kde=True, edgecolor='black')
sns.histplot(df['AirTime'], bins=30, ax=ax5, kde=True, edgecolor='black')

plt.tight_layout()
plt.show()
```

Starting with the `Distance` variable, the histogram shows the distribution is right skewed, with the majority of flights being less than 1000 miles long. The normalized count plot for `DistanceGroup`, which groups flights into distance groups 250 Miles, shows a similar distribution. According to the summary statistics for these variables, the mean distance for flights in the dataset is 735 miles, with a standard deviation of 569 miles, and 75% of flights being less than 969 miles long or within the first 4 distance group. Fewer flights are longer, which may be the case for flights to Hawaii, Alaska, as well as to U.S. territories and possessions and cross-country flights.

The summary statistics and histograms for `CRSElapsedTime`, `ActualElapsedTime`, and `AirTime` show a different story. The distribution for these variables is also right skewed, with 75% of flights lasting under 160 minutes or 2 hours and 40 minutes from departure to arrival. However, the `ActualElapsedTime` and, in particular, `AirTime` variables exhibit negative values, which is not possible for flight times. Two possible explanations for this is that time zone differences, as well as flights spanning across midnight, might have caused the arrival time to be earlier than the departure time. This will require further exploration to confirm, but, if true, would simple require time zone adjustments and simple matrix operations to fix. Finally, the `AirTime` variable has a large amount of missing values, which will require further processing.

We will explore these issues in the following sections, starting with the question of outlying values in the distance variables. Then we will move on to the issue of negative values in the time variables. But first, we will convert the numerical variables to integers.

### Data Cleaning: Changing Data Types

We will convert the numerical variables to integers, as they are all counts or durations, and thus do not require decimal points. To allow while keeping the missing values, we will convert the variables to `Int64` type and ignore missing values using the `errors='coerce'` argument from `pd.to_numeric`.

```{python}
df[sum_cols[:-1]] = df[sum_cols[:-1]].apply(pd.to_numeric, errors='coerce').astype('Int64')
display(df[sum_cols].dtypes)
```

### Exploring Distance Outliers

We will start by exploring flights with outlying distances. To achieve this, we will filter the dataset for flights with distances greater than the 3rd quartile and preview the unique `Origin` and `Dest` value combinations for these flights.

```{python}
long_flights = df.query("Distance > 969").sort_values(by='Distance', ascending=False)
print(f"Number of flights greater than 3Q (969): {long_flights.shape[0]} or {long_flights.shape[0]/df.shape[0]:.2%}")

display(df.groupby(['Origin', 'Dest'])['Distance'].max().reset_index().sort_values(by='Distance', ascending=False).head(10))
```

As suspected, long distances are mostly flights to or from Hawaii. The aggregation above shows that the top 10 longest flights are all to or from Hawaii, with the longest flight being between Honolulu (HNL) and Boston (BOS). According to the histograms from the previous section, these flights are not only outliers, but are also among the least frequent in the dataset.

Below we show which are the top 10 most frequent flights not involving Hawaii. 

```{python}
long_flights_not_hawaii = long_flights.query("~OriginStateName.str.contains('Hawaii') & ~DestStateName.str.contains('Hawaii')")
display(long_flights_not_hawaii.groupby(['Origin', 'Dest'])['Distance'].max().drop_duplicates().reset_index().sort_values(by='Distance', ascending=False).head(10))
```

As we can see, the longest flights not involving Hawaii are mostly flights betwen Alaska (ANC) and east cost cities like Atlanta (ATL), Philadelphia (PHL), Newark (EWR) and Chicago (IAH). Interestingly, the second longest flight in this groups does not involve either Hawaii or Alaska, but a U.S. territory. This is a flight between San Juan (SJU) and Los Angeles (LAX).

Lastly, we leave below the top 10 longest flights not involving Hawaii or Alaska, which, as expected, mostly involve transcontinental flights.

```{python}
long_flights_not_hawaii_alaska = long_flights.query("~OriginStateName.isin(['Hawaii', 'Alaska']) & ~DestStateName.isin(['Hawaii', 'Alaska'])")
display(long_flights_not_hawaii_alaska.groupby(['Origin', 'Dest'])['Distance'].max().drop_duplicates().reset_index().sort_values(by='Distance', ascending=False).head(10))
```

### Data Cleaning: Negative Time Values

In this section, we will explore the issue of negative time values in the `ActualElapsedTime` and `AirTime` variables. We will start by filtering the dataset for flights with negative `ActualElapsedTime` and `AirTime` values and preview the departure and arrival times (both CRS and real) for these flights, along with their origin and destination states to account for time differences.
```{python}
negative_times = df.query("ActualElapsedTime < 0 | AirTime < 0")[['OriginState', 'DestState', 'CRSDepTime', 'CRSArrTime', 'DepTime', 'ArrTime', 'ActualElapsedTime', 'AirTime']]

print(f"Number of negative values in `ActualElapsedTime`: {negative_times.query('ActualElapsedTime < 0').shape[0]}")
print(f"Number of negative values in `AirTime`: {negative_times.query('AirTime < 0').shape[0]}")

display(negative_times.query("ActualElapsedTime < 0"))
display(negative_times.query("AirTime < 0").sample(5))
```

The filtered data shows that the number of negative values in these columns is extremely small (<0.01% of the dataset). It is not clear from the sample data how are the values for `AirTime` calculated, since even with the deleted taxi variables the `AirTime` values are still not consistent with the difference between `WheelsOff` and `WheelsOn`. We decide therefore to drop the `AirTime` variable, as it is not clear how it is calculated and it is not consistent with the rest of the dataset.

The sample data for `ActualElapsedTime` confirms our hypothesis that the negative values are due to flights spanning across midnight and time zone differences. This leads to an even bigger realization: given the time variables do not contain time zone information, but are set in local times, both the `CRSElapsedTime` and `ActualElapsedTime` values do not represent the actual elapsed time for the flights. This is a major issue, since accurate time measurements for flight duraction might be very useful variable for feature engineeering and for our model. We will therefore also drop these features and replace them with an accurate calculation of the elapsed times, accounting for time zone differences.

```{python}
print(f"Columns before drops: {df.shape[1]}")
df.drop(columns='AirTime', inplace=True)
df.drop(columns='CRSElapsedTime', inplace=True)
df.drop(columns='ActualElapsedTime', inplace=True)
sum_cols.remove('AirTime')
sum_cols.remove('CRSElapsedTime')
sum_cols.remove('ActualElapsedTime')
print(f"Columns after drops: {df.shape[1]}")
```

### Feature Engineering: Elapsed Time Calculation

With time zones imputed for all time variables, we can accurately calculate the actual elapsed time for flights by converting the time variables to UTC and calculating the time difference between the `ArrTime_UTC` and `DepTime_UTC` variables. We will first convert the time variables to UTC.

```{python}
for time_col in time_cols:
    df[time_col + '_UTC'] = pd.to_datetime(df[time_col], utc=True)

utc_cols = [col + '_UTC' for col in time_cols]

display(df[time_cols + ['OriginState', 'DestState', 'CRSDepTime_UTC', 'DepTime_UTC', 'CRSArrTime_UTC', 'ArrTime_UTC']].head())
```

Now with the time variables in UTC, we can accurately calculate the actual elapsed time for the flights. We will recreate the `CRSElapsedTime` and `ActualElapsedTime` with the difference between the `ArrTime_UTC` and `DepTime_UTC` variables. 

```{python}
df['CRSElapsedTime'] = ((df['CRSArrTime_UTC'] - df['CRSDepTime_UTC']).dt.total_seconds() / 60).astype('int')
df['ActualElapsedTime'] = ((df['ArrTime_UTC'] - df['DepTime_UTC']).dt.total_seconds() / 60).astype('int')

sum_cols.append('CRSElapsedTime')
sum_cols.append('ActualElapsedTime')

display(df[sum_cols].head())
```

### Value Counts (updated)

Now that we have cleaned the flight summary variables and recalculated our elapsed time variables, we can now analyze the distribution of these variables. Below we preview the summary statistics and plot the elapsed time variables.
```{python}
display(univariate_preview(df, ['CRSElapsedTime', 'ActualElapsedTime']))

fig, axs = plt.subplots(2, 2, gridspec_kw={"height_ratios": (.2, .8)}, figsize=figsize)
axs = axs.ravel()

for i, col in enumerate(['CRSElapsedTime', 'ActualElapsedTime']):
    sns.boxplot(x=df[col], ax=axs[i]).set_xlabel('')
    sns.histplot(df[col], ax=axs[i+2])

x_range = df[['CRSElapsedTime', 'ActualElapsedTime']].values.flatten()
x_min, x_max = x_range.min(), x_range.max()
for ax in axs:
    ax.set_xlim(x_min - 100, x_max + 100)

plt.tight_layout()
plt.show()
```

The summary statistics and plots above show that the distribution of the `CRSElapsedTime` and `ActualElapsedTime` variables are fairly similar, with the IQ range being virtually the same for both variables. The boxplots show that the majority of flights have an elapsed time of less than around 300 minutes, with medians of around 110. This is further visualized by the histograms, which demonstrate that the distributions for these variables are right skewed. 

There is an interesting difference among the outliers between the two variables. The `ActualElapsedTime` variable has both a larger amount and wider range of outliers than the `CRSElapsedTime` variable. Variation between both variables can be expected, due to the role of delays in the actual elapsed time. However, outliers in the `ActualElapsedTime` span up to twice the maximum value of the `CRSElapsedTime` variable. We will explore any possible correlations to explain this difference in our bivariate analysis in the next notebook.

## Delay Summary Variables

We now turn to our last group of variables, the delay summary variables. These include variables explaining the cause of delays, such as `CarrierDelay`, `WeatherDelay`, `NASDelay`, `SecurityDelay`, and `LateAircraftDelay`. It is worth mentioning that these variables are only available from June 2003 onwards. Despite these variables being available for only around half of our dataset's time range, we will still analyze the distribution of these variables, to answer some of the questions we stated for this project.

### Value Counts

We will first filter the dataset for flights after June 2003 with departure or arrival delays above 15 minutes, given this is the industry standard in measuring delays. We will then preview the value counts for the delay summary variables.

```{python}
delay_cause_df = df.loc[(df["FlightDate"] >= '2003-06-01') & ((df["DepDel15"].eq(1)) | (df["ArrDel15"].eq(1)))]

print(f"Percentage of data with delays after June 2003: {delay_cause_df.shape[0]/df.shape[0]:.2%}")

univariate_preview(delay_cause_df, delay_cols)
```

After filtering, we see that around 13% of flights exhibit delays higher than 15 minutes after June 2003. Considering that from the entire dataset 20% of flights exhibit such delays, this demonstrates that 60% of delays are concentrated in the second half of the date range, indicating an increase in delays over time. We will further confirm this in our bivariate analysis in the next notebook.

Moreover, 15% of values are missing across the variables, plausibly suggesting either a lack of information or the lack of an appropriate variable to explain the cause of the delay. This leave us with an effective 11.25% of the original dataset containing information on the cuase for delay. We will definitely discard these variables from our model, however, we will still analyze the distribution of these variables to answer some of the questions we stated at the start of this project.

The summary statistics show that 75% of delays caused by `CarrierDelay` and `NASDelay` are less than 20 minutes, while the 75% of delays are not caused by either `WeatherDelay` or `SecurityDelay`. `LateAricraftDelay` exhibits however the highest 3 quartile range, with causing delays of up to 27 minutes for 75% of flights. These distributions are further visualized in the histograms below.
```{python}
fig, axs = plt.subplots(3, 2, figsize=(20, 15))
axs = axs.ravel()

for i, col in enumerate(delay_cols):
    sns.histplot(delay_cause_df[col], ax=axs[i], bins=50, edgecolor='black')
    axs[i].set_title(col)
    axs[i].set_xlabel(None)

plt.tight_layout()
plt.show()
```

As visualized in the histograms, variables such as `WeatherDelay` and `SecurityDelay` have an overwhelmingly high kurtosis and a very low standard deviation. Meanwhile, `CarrierDelay`, `NASDelay`, and `LateAircraftDelay` have slightly larger standard derivations and markedly lower kurtosis. This indicates that the distribution of these variables is more spread out than the other two. All of these variables exhibit a right skewed distribution, with the vast majority of delays being less than 200 minutes. 

The highest values for these variables reach up to around 1600 minutes, with `SecurityDelay` having the lowest maximum value, causing a delay of 219 minutes. Values above the 75% quartile are definitely outliers and could be interesting to explore. However, due to the amount of missing values and the fact that these variables are only available for a small portion of the dataset, we will remove these variables from our model and analysis.

### Data Cleaning: Removing Delay Summary Variables
```{python}
print(f"Columns before drops: {df.shape[1]}")
df.drop(columns=delay_cols, inplace=True)
print(f"Columns after drops: {df.shape[1]}")
```

## Conclusion

In this notebook, we performed a univariate analysis for the remainder of variables in our dataset. We explored the value counts and distributions of all departure and arrival variables, as well as the flight summary and delay summary variables. We have also cleaned the dataset throughout our exploration, removed unnecessary variables, dealt with missing values, and finally prepared it for our bivariate analysis.

<!-- We have cleaned the dataset and analyzed the distribution of the variables. We have removed unnecessary variables, converted the time variables to `datetime` format, filled missing CRS values, created binary delay variables, and converted numerical variables to integers. We have also imputed time zone information for all time variables, calculated the actual elapsed time for flights, and removed the delay summary variables. We have also explored the distribution of the variables, and have identified several issues that will require further exploration in our next notebook.

In the next notebook, we will explore the relationships between the variables in the dataset, and will analyze the correlation between the variables. We will also explore the outliers in the dataset, and will analyze the distribution of the variables in relation to the delay variables. Finally, we will prepare the dataset for our prediction model, and will export the dataset for further analysis.
 -->

Our univariate analysis has yielded several important insights about the dataset, which will be useful for our bivariate analysis and for our prediction model. We summarized the key insights below.

### Key Insights about the Data

1. **Time Variables**: The distribution of the time variables shows that departures are more prevalent during morning hours, while arrivals are more predominant during the evening. The distribution of CRS and true time variable pairs for departures and arrivals, respectively, are almost identical, revealing that the size of flight delays is small on average across the dataset.

2. **Delay Variables**: The majority of flights are on time, with only around 20% exhibiting some kind of delay. The majority of delayed flights are delayed by 15 minutes or more, with the most frequent delay groups ranging from -15 to 30 minutes. The distribution of delays is right skewed, with more than 40% of flights ahead of schedule by less than 15 minutes and around 30% of flights on time. The distribution has a long, right tail, with an increasingly smaller number of flights experiencing delays greater than 15 minutes. The distribution of the categorical delay variables shows that the majority of delayed flights are delayed by 15 minutes or more.

3. **Flight Summary Variables**: The distribution of the `Distance` variable is right skewed, with the majority of flights being less than 1000 miles long. The distribution for `CRSElapsedTime`, `ActualElapsedTime`, and `AirTime` is also right skewed, with 75% of flights lasting under 160 minutes or 2 hours and 40 minutes from departure to arrival. The `ActualElapsedTime` variable has both a larger amount and wider range of outliers than the `CRSElapsedTime` variable.

4. **Delay Summary Variables**: The distribution of the delay summary variables is right skewed, with the vast majority of delays being less than 200 minutes. The highest values for these variables reach up to around 1600 minutes, with `SecurityDelay` having the lowest maximum value, causing a delay of 219 minutes.

In the next notebook, we will explore the relationships between the variables in the dataset, and will analyze the correlation between the variables. We will also explore the outliers in the dataset, and will analyze the distribution of the variables in relation to the delay variables. Finally, we will prepare the dataset for our prediction model, and will export the dataset for further analysis.

### Exporting Dataset and Variables

```{python}
df.to_parquet('./data/interim/04-airline_2m.parquet', index=False)
```

```{python}
variables = {name: value for name, value in locals().items() if name.endswith('_cols')}

with open('./src/modules/variables_04.py', 'w') as f:
    for name, value in variables.items():
        f.write(f"{name} = {value}\n")
```
